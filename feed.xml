<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <title>Tianhao&#x27;s website</title>
    <link href="https://clpr.github.io/feed.xml" rel="self" />
    <link href="https://clpr.github.io" />
    <updated>2025-05-29T16:04:24+08:00</updated>
    <author>
        <name>Tianhao Zhao</name>
    </author>
    <id>https://clpr.github.io</id>

    <entry>
        <title>Patching utility functions</title>
        <author>
            <name>Tianhao Zhao</name>
        </author>
        <link href="https://clpr.github.io/patching-crra-utility-function.html"/>
        <id>https://clpr.github.io/patching-crra-utility-function.html</id>

        <updated>2025-05-29T15:13:47+08:00</updated>
            <summary>
                <![CDATA[
                    Some utility functions have singularity u(c) --&gt; -infty as c--&gt;0. CRRA utility, which is a good example, is one of the most popular specifications that&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>Some utility functions have singularity <code>u(c) --&gt; -infty</code> as <code>c--&gt;0</code>. CRRA utility, which is a good example, is one of the most popular specifications that are de facto standard in today’s macroeconomic modeling. However, the singularity <code>u(c) --&gt; -infty</code> as <code>c</code> approaching zero brings annoying numerical challenges in practice. Even though in mathematics, such singularity ensures the optimal <code>c &gt;0</code> strictly, the numerical solvers, however, do not understand this implication and may try evaluating the Lagrangian/Hamiltonian at <code>c &gt;= 0</code>. This especially happens when the budget constraint leads to an irregular admissible space of <code>c</code> and one is using interior point solvers.</p><p>A traditional numerical trick is to set a small tolerace <code>d&gt;0</code>:</p><p>$$
u(c) \approx \frac{1}{1-\gamma}(c+d)^{1-\gamma}
$$</p><p>such that some small negative <code>c</code> would not crash the whole program. Of course, the cost of such shifting operation is a little degree of risk aversion twist.</p><p>However, this trick is not good enough: how large should the <code>d</code> be to handle all the millions of grid points in each iteration? If <code>d</code> is too large, can we still claim that this is a CRRA utility?</p><p>Thus, a natural idea is to find a way to “extend” the CRRA utility function to the whole real line, while keeping the true CRRA utility function form without <code>d</code> in the regions where the optimal solution locate. </p><p>To be accurate: by setting a small enough positive threshold <code>m&gt;0</code>, for any <code>c&lt;m</code>, we replace the true CRRA utility function with a smooth extrapolation. The patched CRRA utility function is a piecewise function:</p><p>$$
\hat{u}(c) = \begin{cases}
\frac{1}{1-\gamma}c^{1-\gamma}  , c \geq m \
P(c|m) , c &lt; m
\end{cases}
$$</p><p>The extrapolation part <code>P</code> is strictly monotonic and smooth enough. We call this operation as “patching”, i.e. cut off the undesirable part of the true function at some point then “patch” the removed part with a patching function <code>P</code>. Intuitively, such patching does not affect the final solution when <code>m</code> is small enough regarding the final optimal consumption level. It only provides a large enough evaluable region for the numerical solvers to explore.</p><p>In the following sections, we discuss some widely used utility function forms and their <code>C^2</code> smooth patching.</p><h2 id="one-dimensional-crra">One dimensional CRRA</h2>
<p>OK, let’s do some math. Suppose we want a <code>C^2</code> patch function <code>P</code> for the one-dimensional CRRA utility <code>u(c)</code>. Let’s consider the following polynomial up to the 2nd order:</p><p>$$
P(c|m) := \beta_0 + \beta_1 c + \beta_2 c^2
$$</p><p>The <code>C^2</code> smoothness requries conditions at the boundary <code>c =m</code>:</p><p>$$
P(m|m) = \beta_0 + \beta_1 m + \beta_2 m^2 = \frac{1}{1-\gamma}m^{1-\gamma}
$$</p><p>$$
P_m (m|m)=\beta_1 + 2\beta_2 m = m^{-\gamma} 
$$</p><p>$$
P_{mm}(m|m)=2\beta_2 = -\gamma m^{-\gamma-1}
$$</p><p>Recursively solving the coefficients gives:</p><p>$$
\beta_2 = -\frac{\gamma}{2} m^{-\gamma-1} 
$$</p><p>$$
\beta_1 = (1-m \gamma) m^{-\gamma} 
$$</p><p>$$
\beta_0 = m^{1-\gamma} ( \frac{1}{1-\gamma} - 1 + m\gamma + \frac{\gamma}{2}  )
$$</p><p>The separable CRRA utility used in <code>Jones, C., Midrigan, V. &amp; Philippon, T. (2022, Econometrica)</code> (“Household Leverage and the Recession.”) follows the same structure because of the additivity.</p><p>(more on the way…)</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Understanding Sequence Space Jacobian (SSJ)</title>
        <author>
            <name>Tianhao Zhao</name>
        </author>
        <link href="https://clpr.github.io/understanding-sequence-space-jacobian-ssj.html"/>
        <id>https://clpr.github.io/understanding-sequence-space-jacobian-ssj.html</id>

        <updated>2025-05-12T17:46:50+08:00</updated>
            <summary>
                <![CDATA[
                    This post progressively introduces the idea and applications of sequence space Jacobian (SSJ) in model perturbation methods, specifically how to efficiently compute perfect foresight transition&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                
    <figure class="blockquote">
      <blockquote>This post progressively introduces the idea and applications of sequence space Jacobian (SSJ) in model perturbation methods, specifically how to efficiently compute perfect foresight transition path or saying IRF.</blockquote>
      <figcaption>Tianhao Zhao</figcaption>
    </figure>

  <p>
    
  </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Collocation with the presence of endogeneous Markovian risk</title>
        <author>
            <name>Tianhao Zhao</name>
        </author>
        <link href="https://clpr.github.io/collocation-with-the-presence-of-endogeneous-markovian-risk.html"/>
        <id>https://clpr.github.io/collocation-with-the-presence-of-endogeneous-markovian-risk.html</id>

        <updated>2025-04-29T14:40:25+08:00</updated>
            <summary>
                <![CDATA[
                    Tianhao Zhao The collocation method is widely used in macroeconomic modeling to approximate value functions (or policy functions) through interpolation techniques. This approach offers two&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>Tianhao Zhao</p><h2 id="introduction">Introduction</h2>
<p>The <strong>collocation method</strong> is widely used in macroeconomic modeling to approximate value functions (or policy functions) through interpolation techniques. This approach offers two key advantages:</p><ol>
<li><p>Many interpolation methods provide greater smoothness than piecewise linear interpolation while requiring fewer supporting nodes.</p></li>
<li><p>By projecting the original problem onto a pseudo-linear one in the space of interpolation coefficients, the collocation method enables analytical evaluation of the Jacobian matrix for the approximated system. This Jacobian facilitates the use of gradient-based algorithms, such as Newton’s method, to accelerate convergence.</p></li>
</ol>
<p>Despite its advantages, many textbooks and online materials overlook an important scenario: cases where the transition matrix (often assumed to describe a Markov process governing uncertainty in dynamic programming) is <strong>exogenous but depends on the current state of the system</strong>. This scenario, involving <strong>endogenous risk</strong>, is highly relevant in economic applications. For instance, an individual’s unemployment risk in general equilibrium may depend on the aggregate unemployment rate, which itself evolves as an aggregate state. Addressing such cases requires specific formulations and methodological adjustments to extend the collocation method.</p><p>In this post, I first establish the notation for collocation and outline the construction of the discretized system. I emphasize that endogenous risk, compared to standard exogenous risk, significantly increases the number of equations required to interpolate the expected value function. Specifically, the size of the system expands proportionally to the number of supporting nodes for endogenous states. I then discuss how to solve the resulting pseudo-linear system and compute its Jacobian matrix analytically. Finally, I explore several topics for efficiently implementing the collocation method.</p><h2 id="notation">Notation</h2>
<blockquote>
<p>Hint: </p><ol>
<li>I do my best to label the dimension of each vector and matrix such that one can easily translate the math notations to their programs without wasting time in figuring out potential dimesion mismatches.</li>
<li>A vector space $\mathbb{R}^{n}$ should be always translated to <em>column</em> vector in computer programming.</li>
</ol>
</blockquote>
<p>We consider solving the following time-invariant Bellman equation:</p><p>$$
\begin{aligned}
&amp; v(x) = \max_{c} u(c,x) + \beta \mathbb{E} { v(x’) | x } &amp; \text{(Bellman equation)} \
\text{s.t. }&amp; c \in \mathcal{C} \subseteq \mathbb{R}^{d_c}  &amp; \text{(admissible space)}  \
&amp; x \in \mathcal{X} \subseteq \mathbb{R}^{d} &amp; \text{(state space &amp; state constraints)} \
&amp; v(x) : \mathcal{X} \to \mathbb{R} &amp; \text{(value function)} \
&amp; x := (x_a, x_e) &amp; \text{(state sorting)} \
&amp; x_a \in \mathbb{R}^{d_a}, x_e \in \mathbb{R}^{d_e}, d_a + d_e = d  &amp; \text{(endo &amp; exogenous states split)} \
&amp; x_a’ = \mu(x,c)  &amp; \text{(endo law of motion)}   \
&amp; x_e \sim \text{MarkovProcess}(x)  &amp;\text{(exogenous law of motion)}  \
&amp; \Pr{ x_e’ | x_e } = \Pr{ x’_e | x_e }(x_a)  &amp; \text{(Markovian)}
\end{aligned}
$$</p><p>where $x_a$ is the sub-vector of endogeneous states, $x_e$ is the sub-vector of exogenous states. The whole law of motion of $x$ consists of $\mu(x,c)$ and the Markov process for $x_e$. The exogenous states $x_e$ follow a $k$-state Markov chain (or any process that can be reasonably approximated by such a Markov chain) with transition matrix $P(x_a)\in\mathbb{R}^{k\times k}$. For convenience of algebra, we sort the dimensions of $x$ by having the $d_e$ exogenous states $x_e$ in the very end of the state vector. </p><blockquote>
<p>Remark: The state vector $x$, in general equilibrium, consists of individual states AND aggregate states simultaneously.</p></blockquote>
<p>Here we assume the transition probability $\Pr{x_e’|x_e}(x_a)$ is a function of endogenous states $x_a$. It locates in the generic case in which the “risk” depends on some endogenous states (e.g. in general equilibrium with endogenous unemployment risk). And for convenience, we split the ransition matrix $P(x_a)$ row by row:</p><p>$$
P(x_a) := \begin{bmatrix} p_{x_a}(\cdot|x_e^1) \ p_{x_a}(\cdot|x_e^2) \ \vdots \ p_{x_a}(\cdot|x_e^k) \end{bmatrix} \in \mathbb{R}^{k\times k}, p_{x_a}(\cdot|x^i) \in \mathbb{R}^{1\times k}
$$</p><p>where $p_{x_a}(\cdot|x_e^i)$ is the distribution vector conditional on current exogenous states $x_e^i$.</p><blockquote>
<p>Attention: the transition matrix $P(x_a)$ depends on endogenous states $x_a$ but the matrix itself determines the next period’s exogenous states $x_e$.</p></blockquote>
<blockquote>
<p>Notes: stacking is not necessary to be Cartesian/tensor product. The supporting nodes are not necessary to be uniform or evenly spaced. This assumption allows for sparse grid and other special grids.</p></blockquote>
<blockquote>
<p>Notation: In the following sections, I keep using $x$ to denote an <em>arbitrary point</em> in the state space $\mathcal{X}$, and using $z\in\mathcal{X}$ to denote <em>supporting nodes</em> of the interpolation. A node $z$ must be a point, but a point is not necessary to be a node.</p></blockquote>
<blockquote>
<p>Remark: Supporting nodes are necessary in either local interpolation methods or global methods. In global methods, these nodes are sampled and evaluated at to fit the interpolation.</p></blockquote>
<p>Let’s define stackings:</p><p>$$
\begin{align}
&amp; V(X) := \begin{bmatrix}
v(x^1) \ v(x^2) \ \vdots \ v(x^h)
\end{bmatrix} \in \mathbb{R}^{h\times 1}, V(Z) \in \mathbb{R}^{n\times 1}
\end{align}
$$</p><p>For convenience, let’s also define the stacking of policy $c(x)$, instantaneous utility $u(c,x)$, and the new endogenous states $x_a’$ by its law of motion:</p><p>$$
\begin{align}
&amp; C(X) := \begin{bmatrix}
c(x^1) \ \vdots \ c(x^h)
\end{bmatrix} \in \mathbb{R}^{h\times d_c}, C(Z) \in \mathbb{R}^{n\times d_c} \
% ———
&amp; U(X) := \begin{bmatrix}
u(c(x^1),x^1) \ u(c(x^2),x^2) \ \vdots \ u(c(x^h),x^h)
\end{bmatrix}\in\mathbb{R}^{h\times 1}, U(Z) \in \mathbb{R}^{n\times 1}  \
% ———
&amp; X_a’(X) := \begin{bmatrix}
\mu(x^1,c(x^1)) \ \mu(x^2, c(x^2)) \ \vdots \ \mu(x^h,c(x^h))
\end{bmatrix} = \vec\mu(X,C(X)) \in \mathbb{R}^{h\times d_a}, X’_a(Z) \in \mathbb{R}^{n\times d_a}
\end{align}
$$</p><p>Let $X := [x^1,x^2,\dots,x^h]^T \in \mathbb{R}^{h\times d}$ be an arbitrary $h$ stacking of $d$-dimensional points where each $x^i\in\mathcal{X}$. A special case of the stacking is $Z:=[z^1,z^2,\dots,z^n]^T \in\mathbb{R}^{n\times d}$ which is an $n$ stacking of $d$-dimensional (unique) supporting nodes. </p><p>Let $v(\cdot):\mathbb{R}^{d}\to\mathbb{R}$ be the value function to solve. An interpolant $\hat{v}(\cdot)$ of degree $m-1$ must have the following form:</p><p>$$
\begin{align}
&amp; \hat{v}(x) := \phi(x) \cdot \theta \
&amp; \phi(x) \in \mathbb{R}^{1\times m}, \theta \in \mathbb{R}^{m\times 1}
\end{align}
$$</p><p>where $\phi(\cdot)$ is the basis vector and $\theta$ is the interpolation coefficient vector. Let’s define stakced basis matrix evaluated at $h$ arbitrary points:</p><p>$$
\Phi(X) := \begin{bmatrix} \phi(x^1) \ \phi(x^2) \ \vdots \ \phi(x^h) \end{bmatrix} \in {\mathbb{R}^{h\times m}}, \Phi(Z) \in \mathbb{R}^{n\times m}
$$</p><p>Thus, the stacked interpolated values can be linearly written as:</p><p>$$
\begin{align}
&amp; \hat{V}(X) = \Phi(X) \cdot \theta \
&amp; \hat{V}(Z) = \Phi(Z) \cdot \theta
\end{align}
$$</p><blockquote>
<p>Hints: For (univariate or multivariate) piecewise linear interpolation, $\Phi(Z) = I_n$ and $\theta=V(Z)$ where $I_n$ is the $n\times n$ identity matrix.</p></blockquote>
<p>Now, let’s interpolate and stack the expected value function (or Q-function). The expectation operation is defined as:</p><p>$$
\begin{align}
&amp; \mathbb{E}{ v(x’)|x } := \sum_{j=1}^k \Pr{x_e’|x_e}(x_a) \cdot v(x’) = v_e(x_a’,x_a,x_e) = v_e(x_a’,x) :\mathbb{R}^{d_a} \times \mathbb{R}^{d} \to \mathbb{R}  \
&amp; \approx \hat{v}_e (x_a’,x) := \phi_e(x_a’,x) \cdot \theta_e  \label{eq:017} \
&amp; \phi_e(\cdot) \in \mathbb{R}^{1\times m_e}, \theta_e \in \mathbb{R}^{m_e \times 1}
\end{align}
$$</p><p>where the dimensionality of $v_e$ is doubled to $d_a+d$.</p><blockquote>
<p>Remark: An alternative way of defining the interpolant is $\phi_e(x_a’) \cdot \theta_e(x)$ which fits many scenarios. However, this alternative method requires further interpolation of a <strong>vector</strong>-valued function $\theta_e(\cdot):\mathbb{R}^{d}\to\mathbb{R}^{\zeta}$. Its dimensionality $\zeta$, which depends on the number of supporting nodes, is infeasibly high. Our current implementation of Eq ($\ref{eq:017}$) works better at a cost of $d_a+d$ dimensionality.</p></blockquote>
<blockquote>
<p>Remark: The node set of $\mathcal{X}_a$ can be different from the set of partial $z$ from the $n$ nodes in $\mathcal{X}$, which allows multiple resolution of the interpolation.</p></blockquote>
<p>For convenience, we define the tensor space of endogenous states $\mathcal{A} := \mathcal{X}_a \times \mathcal{X} \subseteq \mathbb{R}^{d_a+d}$ and use notation $a=(x_a’,x)\in\mathcal{A}$ to denote an arbitray joined point; and use $a_z = (z_a’,z)$ to denote a point that is joined by two endogenous state nodes. There are $n_a^2$ such nodes. Then, let’s define the stacked expected value function, and the stacked basis matrix of the expected value function:</p><p>$$
\begin{align}
&amp; V_e(A) : \mathbb{R}^{h\times (d_a+d)} \to \mathbb{R}^{h\times 1}, V_e(A_z): \mathbb{R}^{n_an\times (d_a+d)} \to \mathbb{R}^{n_an\times 1} \
&amp; \Phi_e(A) \in \mathbb{R}^{h \times m_e}, \Phi_e(A_z) \in \mathbb{R}^{n_an \times m_e}
\end{align}
$$</p><p>Thanks to the linearity of the expectation operation, we can define the expectation operator as a row-vector such that:</p><p>$$
\begin{align}
&amp; v_e(x_a’,x) = \sum_{j=1}^k p_{x_a}(e^j|x_e)(x_a) \cdot v(x_a’,e^j) = p_{x_a}(\cdot |x_e) \cdot \begin{bmatrix} v(x_a’, e^1) \ v(x_a’, e^2) \ \vdots \ v(x_a’, e^k) \end{bmatrix}, e^j\in \underbrace{ { x_e^1, \dots, x_e^k } }_{k \text{ Markov states}}
\end{align}
$$</p><p>Intuitively, for the following stacked mapping from node stacking $Z$ to node stacking $A_z$:</p><p>$$
\underbrace{V_e(A_z)}<em>{\mathbb{R}^{n_an\times 1}} = \mathbb{E} \circ \underbrace{V(Z)}</em>{\mathbb{R}^{n\times 1}}
$$</p><p>one can define an expectation matrix $E\in\mathbb{R}^{n_an \times n}$ such that $V_e(A_z)=E\cdot V(Z)$. Each row of $E$ picks needed nodes from the $n$ nodes of $Z$ and does weighted summation using the corresponding distribution vector $p_{x_a}(\cdot|x_e)$ according to the current node of $A_z$.</p><blockquote>
<p>Remark: The construction of $E$, personally speaking, is the hardest part of collocation. Its structure not only depends on the stacking order of your $Z$ nodes but also depends on the type of your grid. Thus, it is hard to write down mathematically (unless some special cases such as Cartesian nodes and states sorted lexicographically or anti-lexicographically), but it might be easy to implement in computer programs using Hash map or other efficient data structures.</p></blockquote>
<hr>
<h2 id="discretized-system">Discretized system</h2>
<p>The discretized system is a pseudo linear system of total $n(n_a+1)$ equations</p><p>$$
\begin{align}
&amp; V(Z) = U(Z) + \beta \cdot V_e(X_a’(Z),Z)  &amp; \text{Bellman equations, total: }n   \
&amp; V_e(A_z) = E \cdot V(Z)   &amp; \text{Q equations, total: } n_an
\end{align}
$$</p><p>accompoanied by $n$ optimization problems (optimization step):</p><p>$$
X_a’(Z) = \vec\mu(Z,C(Z)), C(Z) = \arg\max_C \text{Hamiltonian(Z)}
$$</p><blockquote>
<p>Remark: </p><ol>
<li>The number of equations degenerates to $n+n_a$ if $P(x_a)$ is no longer depending on endogeneous states $x_a$. This happens in stationary equilibrium or purely exogenous risk.</li>
<li>The endogeneity of the risk $P$ dramatically increases the number of equations by approximately $n_a$ times.</li>
</ol>
</blockquote>
<p>Plugging the interpolation in:</p><p>$$
\begin{align}
&amp; \Phi(Z) \cdot \theta = U(Z) + \beta \cdot \Phi_e(X’_a(Z),Z) \cdot \theta_e  \label{eq:030} \
&amp; \Phi_e(A_z) \cdot \theta_e = E \cdot \Phi(Z) \cdot \theta  \nonumber
\end{align}
$$</p><p>There are $m+m_e$ equations about the inteprolation coefficients $\vec\theta:=(\theta,\theta_e)$. </p><blockquote>
<p>Remark: </p><ol>
<li>For both global and local interpolation methods, $m+m_e$ is usually equal to $n(n_a+1)$.</li>
<li>Thus, if there is no discontinuity of $v$, then global methods are always more preferred.</li>
</ol>
</blockquote>
<p>The system is pseudo linear in the coefficients. To solve this system using gradient-based methods such as Newton’s method, we define:</p><p>$$
\begin{align}
&amp; \mathbf{F}(\vec\theta) := \text{RHS}(\vec\theta) - \text{LHS}(\vec\theta) = 0 \
&amp; \text{RHS}(\vec\theta) := \begin{bmatrix}
U(Z) + \beta \cdot \Phi_e(X’_a(Z),Z) \cdot \theta_e \
E \cdot \Phi(Z) \cdot \theta
\end{bmatrix} \
&amp; \text{LHS}(\vec\theta) := \begin{bmatrix}
\Phi(Z) \cdot \theta \
\Phi_e(A_z) \cdot \theta_e
\end{bmatrix}
\end{align}
$$</p><blockquote>
<p>Remark: The Eq ($\ref{eq:030}$) can be converted to a fixed-point format by left multiplying $\Phi^{-1}(Z)$ and $\Phi^{-1}_e(A_z)$ to both sides of the equation. The fixed-point format can be directly used to run some iterative algorithms.</p></blockquote>
<p>There is Jacobian:</p><p>$$
\begin{align}
\nabla \mathbf{F}(\vec\theta) =&amp; \begin{bmatrix}
\mathbf{0}<em>{m\times m} &amp; \beta \cdot \Phi_e(X’<em>a(Z),Z) \
E \cdot \Phi(Z)  &amp; \mathbf{0}</em>{m_e \times m_e}
\end{bmatrix} - \begin{bmatrix}
\Phi(Z)  &amp; \mathbf{0}</em>{m_e\times m_e} \
\mathbf{0}_{m\times m} &amp; \Phi_e(A_z)
\end{bmatrix}  \nonumber  \
% ——-
=&amp; \begin{bmatrix}
-\Phi(Z) &amp; \beta \cdot \Phi_e(X’_a(Z),Z) \
E \cdot \Phi(Z)  &amp; -\Phi_e(A_z)
\end{bmatrix}  \label{eq:034}
\end{align}
$$</p><p>With the analytical Jacobian, one can efficiently solve the whole problem in a reasonable time.</p><blockquote>
<p>Remark: One can quickly figure out that most terms of $\nabla \mathbf{F}(\vec\theta)$ can be pre-conditioned except the basis matrix $\Phi_e(X_a’(Z),Z)$. The reason is that the new endogenous states $X_a’(Z)$ change by iteration and evaluating it requires doing the optimization step. The optimization step, which also determines $U(Z)$ of every iteration in $\mathbf{F}(\vec\theta)$, is the <em>bottleneck</em> of the whole algorithm’s performance.</p></blockquote>
<h2 id="topic-chocie-of-interpolation-methods">Topic: Chocie of interpolation methods</h2>
<p>The choice of interpolation methods becomes particularly important as the dimensionality of the Bellman equation increases. Since the number of interpolation coefficients must match the number of supporting nodes to exactly determine the coefficients, the projected pseudo-linear system remains subject to the curse of dimensionality (not only the number of states, but also the extra dimensionality due to the endogenous risk!). Thus, one must carefully trade off between different interpolation methods based on their performance characteristics. The following table compares the memory demand of local and global interpolation methods generally given the same accuracy demand.</p><table>
<thead>
<tr>
<th>Method</th>
<th>Required number of nodes</th>
<th>Basis matrix sparsity</th>
</tr>
</thead>
<tbody><tr>
<td>Local</td>
<td>Large</td>
<td>Sparse</td>
</tr>
<tr>
<td>Global</td>
<td>Not that large</td>
<td>Dense (usually)</td>
</tr>
</tbody></table>
<p>The following table compares some popular interpolation methods:</p><table>
<thead>
<tr>
<th><strong>Method</strong></th>
<th><strong>Type</strong></th>
<th><strong>Time Complexity</strong></th>
<th><strong>Memory Complexity</strong></th>
<th><strong>Notes</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>Piecewise Linear</strong></td>
<td>Local</td>
<td>$O(n)$</td>
<td>$O(n)$</td>
<td>Simple, fast, but less smooth; scales well for higher dimensions.</td>
</tr>
<tr>
<td><strong>Cubic Spline</strong></td>
<td>Local</td>
<td>$O(n)$</td>
<td>$O(n)$</td>
<td>Provides smoother approximations than linear interpolation but may struggle in very high dimensions.</td>
</tr>
<tr>
<td><strong>Polynomial Basis</strong></td>
<td>Global</td>
<td>$O(n^3)$</td>
<td>$O(n^2)$</td>
<td>Highly smooth but computationally expensive; impractical for high dimensions.</td>
</tr>
<tr>
<td><strong>Chebyshev Polynomials</strong></td>
<td>Global</td>
<td>$O(n \log n)$</td>
<td>$O(n)$</td>
<td>Offers better numerical stability and smoothness; suitable for moderate dimensions.</td>
</tr>
<tr>
<td><strong>Radial Basis Functions (RBF)</strong></td>
<td>Global</td>
<td>$O(n^2)$</td>
<td>$O(n^2)$</td>
<td>Flexible and accurate but memory-intensive; more suitable for low-dimensional problems.</td>
</tr>
</tbody></table>
<p>One can quickly realize that the trade-off does not break the curse of dimensionality. To really break it, one can explore other dimensionality reduction techniques such as</p><ul>
<li><strong>Sparse grid</strong>: This method uses the idea of optimizing the grid structure. It reduces the number of nodes while keeping the sparsity of the basis matrix. If incorporating node adaption, then the number of required nodes can be further cut</li>
<li><strong>Machine learning</strong>: This method employs a highly efficient interpolation/approximation technique, where automatic differentiation and backpropagation can significantly accelerate the iteration process. In fact, it represents a generalized approach to the traditional collocation method.</li>
</ul>
<h2 id="topic-recipe-of-newtons-method">Topic: Recipe of Newton’s method</h2>
<p>To solve Eq ($\ref{eq:030}$), one can call any standard solvers (e.g. MATLAB’s optimization toolbox) and provide Eq ($\ref{eq:034}$) to the solver. However, when someone has to write their own iterator, the following simple updating formula is always good to recall:</p><p>$$
\vec\theta’ = \vec\theta - \left(\nabla \mathbf{F}(\vec\theta)  \right)^{-1} \cdot \mathbf{F}(\vec\theta)
$$</p><p>To improve the efficiency by avoiding inverting the Jacobian matrix when $m+m_e$ is large, one can check some alternative methods such as <strong>Newton-Krylov Method</strong>.</p><h2 id="topic-optimization-step">Topic: Optimization step</h2>
<p>Even though collocation significantly accelerates the convergence of the entire iteration, <strong>90% of the computational cost</strong> is still concentrated in the <strong>optimization step</strong>, where one must solve $m + m_e$ embarrassingly differentiated optimization problems in <em>every</em> iteration to get the instantaneous utility $U(C(Z),Z)$ and new endogenous states $X_a’(Z)$.  </p><p>The performance depends heavily on the number of control variables and the complexity of the admissible space $\mathcal{C}$:  </p><ul>
<li>It is <strong>fast</strong> if there is only <strong>one control variable</strong> (e.g., consumption) and a <strong>linear search</strong> can handle it efficiently.  </li>
<li>It is <strong>manageable</strong> with <strong>two control variables</strong> (e.g., consumption and leisure), where the <strong>simplex method</strong> can still work reasonably well in unconstrained case and with space rectangularization.</li>
<li>It becomes <strong>very slow</strong> when there are <strong>more than three control variables</strong>, especially if constraints make the admissible space $\mathcal{C}$ <strong>non-convex</strong>. And often, even converngence is ensured, the smoothness of $C(Z)$ and $X_a’(Z)$ is not ensured.</li>
</ul>
<p>Therefore, one must carefully select the optimizer while verifying the interpolation properties used in collocation. For example, the MATLAB function <code>fmincon</code> is powerful and robust, but it may fail to converge within a reasonable time if the interpolation method causes numerical oscillations or destroys the convexity of the Hamiltonian.  </p>
            ]]>
        </content>
    </entry>
</feed>
