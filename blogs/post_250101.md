# Specifying the IPNewton method in `Optim.jl`

Tianhao Zhao

[TOC]

## Introduction

`Optim.jl` is a powerful Julia package that offers a wide range of optimization algorithms. Among these, interior-point Newton (`IPNewton`) is the only gradient-based method capable of solving non-linearly constrained problems:
$$
\begin{align}
& \min_{x} f(x)  \label{eq:01} \\
\text{s.t. }& l_x \leq x \leq u_x & \text{(box constraints)} \\
& l_c \leq c(x) \leq u_c  & \text{(non-linear constraints)} \\
& x, l_x, u_x \in\mathbb{R}^{n}   \\
& c(x), l_c, u_c \in\mathbb{R}^{p} 
\end{align}
$$

where $n$ represents the number of control variables and $p$ denotes the number of non-linear inequality constraints (with equality constraints satisfying $l_c = u_c$).

Despite its flexibility, `IPNewton` can magically accelerate the algorithm by requiring users to provide the Jacobians and Hessians for both objective function $f(x)$ and constraint functions $c(x)$. Even though automatic differentiation is available today, user-provided information still has the best performance when it is available. However, specifying these data structures and functions is challenging because of the math complexity and the non-transparent internal routines of the algorithm.

> Notes: Automatic differentiation is available by option `autodiff=true` and implemented by `ForwardDiff.jl`

In this post, I clearly outline the ideas behind and math formulas for these data structures (primarily matrices) to help readers better understand the [official documentation](https://julianlsolvers.github.io/Optim.jl/stable/examples/generated/ipnewton_basics/#Optimization-interface) and tailor the method to their specific problems.

## What to provide

> For readability, we define the following alias:
>
> - `F64 = Float64`
> - `Vec = AbstractVector`
> - `Mat = AbstractMatrix`
> - `V64 = Vector{Float64}`
> - `M64 = Matrix{Float64}`
> - `Num = Union{Real, Int, ...}`

To solve the problem of Eq ($\ref{eq:01}$) using `IPNewton` API, one need to manually specify the following functions:

| Definition                             | Math                                     | Function declaration                      | Parameters                                                   | In-place? |
| -------------------------------------- | ---------------------------------------- | ----------------------------------------- | ------------------------------------------------------------ | --------- |
| Objective function                     | $f(x)\in\mathbb{R}$                      | `fun(x::Vec)::Num`                        | control variable vector `x` of length `n`                    | False     |
| Jacobian of objective function         | $\nabla f(x)\in\mathbb{R}^{n}$           | `fun_grad!(g::Vec,x::Vec)::Nothing`       | gradient vector `g` of length `n` and control vector `x`     | True      |
| Hessian of objective function          | $\nabla^2 f(x)\in\mathbb{R}^{n\times n}$ | `fun_hess!(h::Mat,x::Vec)::Nothing`       | hessian matrix `h` and control vector `x`                    | True      |
| Non-linear constraints                 | $c(x)\in\mathbb{R}^{p}$                  | `con_c!(c::Vec,x::Vec)::Vec`              | constraint residuals `c` and control `x`                     | Partially |
| Jacobian of the non-linear constraints | $\nabla c(x)\in\mathbb{R}^{p\times n}$   | `con_jacobian!(J::Mat,x::Vec)::Mat`       | Jacobian matrix `J` and control                              | Partially |
| Hessian of the non-linear constraints  | $\nabla^2 c(x)\in\mathbb{R}^{n\times n}$ | `con_h!(h::Mat,x::Vec,lam::Vec)::Nothing` | Hessian matrix `h` of ==objective== `f`, control `x`, $\lambda$-weight `lam` of length $p$ | True      |

## Illustrative example

Let's illustrate how to specify these functions using a non-trivial example of economic modeling:
$$
\begin{align}
 v(a,h,z) =& \max_{C,a',h',\ell} \space \log{C} + \gamma\log{h} - \delta \log{\ell} + \beta \cdot \underbrace{ \int_{z'|z} v(a',h',z') d \phi(z'|z) }_{=: \bar{v}(a',h',z)} \\
\text{s.t. }& C + (1+r)a + p h' = zh^\alpha \ell^{1-\alpha} + a' + p h \\
& C \geq 0  &\text{(Non-negative consumption)} \\
& a'\geq 0  & \text{(No shorting of asset)} \\
& h' \geq 0 &\text{(No-shorting of housing)} \\
& \ell \in [0,1] &\text{(Time endowment)} \\
& a' \leq \theta p h'  & \text{(Collateral constraint)} \\
& (a,h,z) \in [\underline{a},\bar{a}] \times [\underline{h},\bar{h}] \times [\underline{z},\bar{z}]  &\text{(State constraints)}
\end{align}
$$

> Notes: to avoid confusion, I use $C$ to denote consumption but use $c$ to denote inequality constraints.

> Notes: Box constraints on the control variables are separately processed by the solver API, so there is no need to convert them into some inequality constraints of $c(x)$.

> Notes: The `IPNewton` API allows custom but constant lower bounds and uppwer bounds for $c(x)$. This may help to simplify the definition of some $c(x)$ constraints.

This is a typical "expert" who produces consumption goods using housing wealth and labor. The expert also borrows in form of bond $a'$ at rate and face collateral constraint that depends on $h'$. This problem has many features that are common in many models. Meanwhile, we also make some technical assumptions regarding computer programming:

1. When solving this problem, the expected value function $\bar{v}(a',h',z)$ has been interpolated. It is wrapped up as a callable function such that we do not need to explicitly write the expectation operation over $v(a,h,z)$.
2. The callable interpolant of $\bar{v}(\cdot)$ can smoothly handle any point in $\mathbb{R}^3$. If a point is not in the feasible state space, then $\bar{v}(\cdot)$ can still return a number (e.g. a very negative number).

Reformulating the problem to a ==minimization== one and cancelling out $c$ using the equality constraint, we re-write the problem as:
$$
\begin{align}
& \min_{x:=(a',h',\ell)} f(x|M) \\
\text{s.t. }& \begin{bmatrix}
\underline{a} \\ \underline{h} \\ 0
\end{bmatrix} \leq x \leq \begin{bmatrix}
\bar{a} \\ \bar{h} \\ 1
\end{bmatrix} &\text{(Box constraints of the controls)} \\
% -----
& \begin{bmatrix}
	0 \\ 0
\end{bmatrix} \leq \underbrace{\begin{bmatrix}
	C(x) - \epsilon \\ 
	\theta p h' - a'
\end{bmatrix}}_{c(x)} \leq \begin{bmatrix}
	+\infty \\
	+\infty
\end{bmatrix} & \text{(Inequality constraints)}
\end{align}
$$
where $M$ is the information set we have for this problem (including current states, parameters, computational space, and other model details); $\epsilon$ is a small amount to avoid exact zero; the substituted consumption $C(x)$ is:
$$
C(x) := zh^\alpha \ell^{1-\alpha} + a' + p h  - (1+r)a - p h'
$$
and the objective function is:
$$
f(x|M) := \log{\left\{ C(x)  \right\}} + \gamma \log{h} - \delta \log{\ell} + \beta \bar{v}(a',h',z)  \label{eq:020}
$$

## Explain by part

The official documentation of `Optim.jl` does not explicitly explain the pipeline of `IPNnewton`. Thus, it is a little bit less intuitive to understand the behavior of required functions. In short, a pipeline is like:

1. User provides a pure objective function $f(x)$ and a pure constraint function $c(x)$
2. The algorithm allocates Jacobians and Hessians for $f(x)$ and $c(x)$ internally
3. Users provide "pipe" functions that receive and modify these internally created Jacobian and Hessian objects *in-place* without manually managing their construction, use, and destruction.

### Objective function

The objective function $f(x)$ is set up as a scalar function call `fun(x::Vec)::Num` that receives a $n$-vector and returns a scalar of the function value. This is standard and required even if the autodiff is turned on.

> Hint: If your objective function requires extra parameters, then consider wrapping it with the information set $M$ as a closure.

In the minimization API, the objective function is the **negative** of Eq ($\ref{eq:020}$).

### Jacobian of objective function

The Jacobian $\nabla f(x)$ is a *column* vector:
$$
\nabla f(x) := \begin{bmatrix}
\frac{\partial f(x)}{\partial x_1} \\ \vdots \\ \frac{\partial f(x)}{\partial x_n}
\end{bmatrix} \in \mathbb{R}^{n}
$$
In the illustrative example:
$$
\nabla f(x) = \begin{bmatrix}
	\partial f(x)/\partial a' \\
	\partial f(x)/\partial h' \\
	\partial f(x)/\partial \ell \\
\end{bmatrix} = \begin{bmatrix}
	\frac{1}{C(x)} + \beta \frac{\partial \bar{v}(a',h',z)}{\partial a'} \\
	\beta \frac{\partial \bar{v}(a',h',z)}{\partial h'} \\
	\frac{1}{C(x)}\cdot z (1-\alpha) h^\alpha \ell^{-\alpha} - \frac{\delta}{\ell}
\end{bmatrix}
$$
The function `fun_grad!(g,x)` should modify each element of vector `g` using the values in the above equation at exactly the same position. This function only updates `g` without returning it.

> Note: One can quickly notice that: ==evaluating $\nabla f(x)$ requires computing the partial derivatives of the expected value function $\bar{v}$==. This can be done explicitly (e.g. if you are using Chebyshev polynomials) or using finite difference approximation (most cases), or using automatic differentiation on the callable function $\bar{v}(\cdot)$. Readers may have to write the difference program themselves and properly handle the case on the boundary of feasible domains. This idea also applies to all the following matrices.

### Hessian of objective function

The Hessian $\nabla^2 f(x)$ is an $n\times n$ matrix:
$$
\begin{align}
\nabla^2 f(x) = \begin{bmatrix}
\frac{\partial^2 f(x)}{\partial x_1 \partial x_1} & \frac{\partial^2 f(x)}{\partial x_1 \partial x_2} & \dots & \frac{\partial^2 f(x)}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f(x)}{\partial x_2 \partial x_1}  & \ddots   & & \frac{\partial^2 f(x)}{\partial x_2 \partial x_n} \\
\vdots    &&& \vdots \\
\frac{\partial^2 f(x)}{\partial x_n \partial x_1}  & \dots & & \frac{\partial^2 f(x)}{\partial x_n \partial x_n}
\end{bmatrix}
\end{align}
$$
In the illustrative example, this matrix looks like:
$$
\begin{align}
\nabla^2 f(x) = \begin{bmatrix}
-\frac{1}{C^2(x)}	+ \beta \frac{\partial^2 \bar{v}(a',h',z)}{\partial a' \partial a'} & \frac{p}{C^2(x)} + \beta \frac{\partial^2 \bar{v}(a',h',z)}{\partial a' \partial h'} & -\frac{z(1-\alpha)h^\alpha \ell^{-\alpha} }{C^2(x)} \\
	\beta \frac{\partial^2 \bar{v}(a',h',z)}{\partial h' \partial a'} & \beta \frac{\partial^2 \bar{v}(a',h',z)}{\partial h' \partial h'} & 0 \\
	-\frac{1}{C^2(x)} \cdot z(1-\alpha)h^\alpha \ell^{-\alpha} & \frac{p}{C^2(x)} \cdot z(1-\alpha)h^\alpha \ell^{-\alpha} &  \frac{z(1-\alpha)(-\alpha)}{C(x)}h^\alpha \ell^{-\alpha-1} - \left[ \frac{z(1-\alpha)h^\alpha \ell^{-\alpha}}{C(x)}  \right]^2 + \frac{\delta}{\ell^2}
\end{bmatrix}
\end{align}
$$
The function `fun_hess!(h::Mat,x::Vec)` should modify each element of matrix `h` using the values in the above equation at exactly the same position. This function updates `h` without returning it.

### Non-linear constraints

The non-linear constraints $c(x)$ is programmed as a vector-value function. The function `con_c!(c::Vec,x::Vec)::Vec` should modify the passed `c` vector and also return `c`. (Wierd behavior uhmmmm)

### Jacobian of the non-linear constraints

Since $\nabla c(x)$ is a vector-value function, its Jacobian is now a *matrix* looking like:
$$
\begin{align}
\nabla c(x) = \begin{bmatrix}
\frac{\partial c_1(x)}{\partial x_1}  & \frac{\partial c_1(x)}{\partial x_2} & \dots & \frac{\partial c_1(x)}{\partial x_n} \\
\frac{\partial c_2(x)}{\partial x_1} & \ddots & \frac{\partial c_2(x)}{\partial x_n} \\
\vdots &&\ddots& \vdots \\
\frac{\partial c_p(x)}{\partial x_1} & \dots && \frac{\partial c_p(x)}{\partial x_n}
\end{bmatrix}  \in\mathbb{R}^{p\times n}
\end{align}
$$
The function `con_jacobian!(J::Mat,x::Vec)::Mat`, however, now not only modifies the passed `J` matrix but returns `J` as well. ==Retuning a copy is the biggest difference of constraint-related API cp. objective-related API.==

In the illustrative example, such a Jacobian looks like:
$$
\begin{align}
\nabla c(x) = \begin{bmatrix}
1 & -p & z (1-\alpha)h^\alpha \ell^{-\alpha} \\
-1 & \theta p & 0
\end{bmatrix}  \label{eq:024}
\end{align}
$$

### Hessian of the non-linear constraints

The `IPNewton` algorithm enforces the inequality constraints using log-barrier functions. It optimizes an augmented Lagrangian with barrier terms. Thus, the total Hessian includes:
$$
H(x) = \nabla^2 f(x) + \sum_{i=1}^{p} \lambda_i \cdot \nabla^2 c_i(x)
$$
which adds extra $p$ Hessian of the constraints on the top of the orginal Hessian of the objective function. This formula is the key to understand the behavior of function `con_h!(h::Mat,x::Vec,lam::Vec)::Nothing`.

- `h::Mat` is the Hessian of the **objective function** $f(x)$ rather than the Hessian of the constraints $c(x)$. This object is internally created by the `IPNewton` API and we users don't need to manage its construction and destruction.
- `lam::Vec` is a $p$-element vector of the Lagangian multipliers $\lambda_i$. This object is also internally created by the API and we just take it as given

The function `con_h!()` looks like:

```julia
function con_h!(h,x,lam)
	# add the hessian of the 1st constraint on the top of `h`; multiplied by the 1st multiplier `lam`
  h += lam[1] * hess_cons_1(x)  # hess_cons_1(x)::Matrix, n*n
  
  # add the hessian of the 2nd constraint on the top of `h`
  h += lam[2] * hess_cons_2(x)
  
  # ...
  
  return nothing
end
```

In the illustrative example, each constraint's Hessian wrt the control $x$ are:
$$
\begin{align}
&\nabla^2 c_1(x) = \begin{bmatrix}
	0 & 0 & 0 \\
	0 & 0 & 0 \\
	0 & 0 & z(1-\alpha)(-\alpha)h^\alpha \ell^{-\alpha-1}
\end{bmatrix}  \label{eq:026} \\
&\nabla^2 c_2(x) = \begin{bmatrix}
  0 & 0 & 0 \\
  0 & 0 & 0 \\
  0 & 0 & 0 
\end{bmatrix} \\
\end{align}
$$
which are derived from the 1st and 2nd row of the Jacobian in Eq ($\ref{eq:024}$) respectively.

> Hint: Let's be more specific, step by step. The first row of Eq ($\ref{eq:024}$) is the Jacobian of the 1st constraint $c_1(x):=C(x) -\epsilon$ :
> $$
> \begin{align}
> \nabla c_1(x) = \begin{bmatrix}
> 1 & -p & z (1-\alpha)h^\alpha \ell^{-\alpha} 
> \end{bmatrix}
> \end{align}
> $$
> A typical Jacobian is a *column* vector so we transpose the above matrix row to:
> $$
> \begin{align}
> \nabla c_1(x) = \begin{bmatrix}
> 1 \\ -p \\ z (1-\alpha)h^\alpha \ell^{-\alpha} 
> \end{bmatrix} = \begin{bmatrix}
> \frac{\partial c_1(x)}{\partial a'} \\ \frac{\partial c_1(x)}{\partial h'} \\ \frac{\partial c_1(x)}{\partial \ell}
> \end{bmatrix}
> \end{align}
> $$
>
> Then, we take derivatives again to get the Hessian $\nabla^2 c_1(x)$ of the 1st constraint as Eq ($\ref{eq:026}$)

## Appendix: Autodiff of $\bar{v}$

The evaluation of Jacobians and Hessians requires approximating the Jacobian and Hessian of the expected value function $\bar{v}(a',h',z)$.

Suppose we have an guess of $\bar{v}(\cdot)$. Suppose we have interpolated the guess and wrapped it as a callable object, which means that it can be called like a function that returns the value of $\bar{v}(\cdot)$ at a given point $x'$:
```julia
xp = [ap,hp,z] # a', h', z
val = ev(xp)
```

The `ForwardDiff.jl` package can perform automatic differentiation on such a callable object. For example, evaluting Jacobian and Hessian at point $[1.0,0.9,0.8]$:
```julia
import ForwardDiff as fd

x0 = [1.0,0.9,0.8]

# Jacobian: 3-vector
fd.gradient(ev, x0)::Vector{Float64}

# Hessian: 3*3 matrix
fd.hessian(ev, x0)::Matrix{Float64}
```

Users may check the GitHub page of the package for more details.